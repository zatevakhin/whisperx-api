---
services:
  whisperx-api:
    build:
      context: .
      dockerfile: gpu.Dockerfile
    image: whisperx-api:latest
    environment:
     - HF_AUTH_TOKEN=""
     - DEVICE=cuda
     - COMPUTE_TYPE=float16
     - MODEL_DIR=/models
     - WHISPER_MODEL=large-v3-turbo
    ports:
      - "8000:8000"
    volumes:
      - data:/models
    restart: unless-stopped
    healthcheck:
      test: curl http://localhost:8000 || exit 1
      interval: 60s
    deploy:
      resources:
        reservations:
          devices:
          - driver: cdi
            capabilities: [gpu]
            device_ids:
            - nvidia.com/gpu=all

volumes:
  data:
